{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "cal8N4jPGmku"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "DATA_PATH = \"../data/exercise_2_shap.csv\"       # dataset location\n",
        "LABEL_COL = \"adherence_predicted\"       # binary target (0/1)\n",
        "\n",
        "FEATURE_COLS = [\n",
        "   \"num_past_iits\", \"prev_iit_status\", \"past_encounters\",\n",
        "    \"CD4_Count\", \"Viral_Load\", \"Current_WHO_HIV_Stage\",\n",
        "    \"time_since_diagnosis_at_scheduled_appointment\",\n",
        "    \"age_at_encounter\", \"gender\", \"Current Regimen Line\",\n",
        "    \"TPT Outcome\", \"NCDs\", \"Establishment\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Rln5A1YgGhC6"
      },
      "outputs": [],
      "source": [
        "def load_data(path: str) -> pd.DataFrame:\n",
        "    df = pd.read_csv(path)\n",
        "    if LABEL_COL not in df.columns:\n",
        "        raise ValueError(f\"Label column '{LABEL_COL}' not found in dataset.\")\n",
        "    return df\n",
        "\n",
        "def thin_to_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # Keep only curated features + label; ignore any that might be missing\n",
        "    present = [c for c in FEATURE_COLS if c in df.columns]\n",
        "    missing = [c for c in FEATURE_COLS if c not in df.columns]\n",
        "    if missing:\n",
        "        print(f\"[info] missing features not in dataset (skipped): {missing}\")\n",
        "    if len(present) < 8:\n",
        "        raise ValueError(f\"Too few selected features present ({len(present)}). Expected up to 15.\")\n",
        "    cols = present + [LABEL_COL]\n",
        "    return df[cols].copy()\n",
        "\n",
        "def prepare_xy(df: pd.DataFrame):\n",
        "    X = df.drop(columns=[LABEL_COL], errors=\"ignore\")\n",
        "    y = df[LABEL_COL].astype(int)\n",
        "\n",
        "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\"]\n",
        "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "    return X, y, num_cols, cat_cols\n",
        "\n",
        "def build_model(num_cols, cat_cols) -> Pipeline:\n",
        "    preproc = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", SimpleImputer(strategy=\"most_frequent\"), num_cols),\n",
        "            (\"cat\", Pipeline(steps=[\n",
        "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "                (\"ohe\",  OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
        "            ]), cat_cols),\n",
        "        ],\n",
        "        remainder=\"drop\",\n",
        "        verbose_feature_names_out=True,  \n",
        "    )\n",
        "\n",
        "    clf = RandomForestClassifier(\n",
        "        n_estimators=300,\n",
        "        class_weight=\"balanced\",\n",
        "        n_jobs=-1,\n",
        "        random_state=RANDOM_STATE,\n",
        "    )\n",
        "\n",
        "    pipe = Pipeline(steps=[\n",
        "        (\"preproc\", preproc),\n",
        "        (\"rf\", clf),\n",
        "    ])\n",
        "    return pipe\n",
        "\n",
        "def print_metrics(y_true, y_pred, header=\"Test Metrics\"):\n",
        "    print(f\"\\n=== {header} ===\")\n",
        "    pr, rc, f1, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average=\"macro\", zero_division=0\n",
        "    )\n",
        "    print(f\"Macro  P: {pr:.3f} | R: {rc:.3f} | F1: {f1:.3f}\")\n",
        "    pr_w, rc_w, f1_w, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average=\"weighted\", zero_division=0\n",
        "    )\n",
        "    print(f\"Weight P: {pr_w:.3f} | R: {rc_w:.3f} | F1: {f1_w:.3f}\")\n",
        "    print(\"\\nClassification report:\\n\")\n",
        "    print(classification_report(y_true, y_pred, zero_division=0))\n",
        "\n",
        "def train_and_eval(df: pd.DataFrame):\n",
        "    X, y, num_cols, cat_cols = prepare_xy(df)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.25, random_state=RANDOM_STATE, stratify=y\n",
        "    )\n",
        "\n",
        "    pipeline = build_model(num_cols, cat_cols)\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate on TEST set \n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    print_metrics(y_test, y_pred, header=\"Test Metrics\")\n",
        "\n",
        "    preproc = pipeline.named_steps[\"preproc\"]\n",
        "    try:\n",
        "        feature_names = preproc.get_feature_names_out()\n",
        "    except Exception:\n",
        "        feature_names = np.array(num_cols + [f\"{c}__OHE\" for c in cat_cols])\n",
        "\n",
        "    return pipeline, X_train, X_test, y_train, y_test, feature_names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Test Metrics ===\n",
            "Macro  P: 0.995 | R: 0.985 | F1: 0.990\n",
            "Weight P: 0.992 | R: 0.992 | F1: 0.992\n",
            "\n",
            "Classification report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99        91\n",
            "           1       1.00      0.97      0.99        34\n",
            "\n",
            "    accuracy                           0.99       125\n",
            "   macro avg       0.99      0.99      0.99       125\n",
            "weighted avg       0.99      0.99      0.99       125\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_raw = load_data(DATA_PATH)\n",
        "df = thin_to_features(df_raw)\n",
        "\n",
        "pipeline, X_train, X_test, y_train, y_test, feature_names = train_and_eval(df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Continue SHAP analysis from here ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
